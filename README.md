# False Premise Hallucination in LLMs

**Project by Anbu Vinotha S**  
A BTech AI & Data Science initiative to investigate and address false premise hallucinations in large language models (LLMs), enhancing their reliability in educational contexts.

## Project Overview
- **Goal**: Evaluate how LLMs respond to false premise questions (e.g., "Why is 2+2=5?") and explore mitigation strategies like prompt engineering.
- **Purpose**: Improve AI accuracy in statistical education.
- **Tools**: Python for analysis, Git for version control.

## Project Structure
- `research_notes`: Contains initial notes on LLM fundamentals and hallucinations.
- `data`: Will hold datasets for experiments (e.g., statistics-based false premise questions).
- `code`: Will include Python scripts for testing LLMs.
- `results`: Will store analysis and visualizations.

## Progress
- **Day 1**: Set up repository and documented basic LLM concepts and hallucination behavior.
- **Day 2**: Planned to summarize three research papers on hallucination mitigation.

## Getting Started
1. Clone the repository:  
   `git clone https://github.com/yourusername/False-Premise-Hallucination-Project`
2. Install required packages:  
   `pip install pandas numpy matplotlib`



## Contact Information
- **Email**: [your.email@example.com]
- **LinkedIn**: www.linkedin.com/in/anbuvinotha

## Last Updated
September 24, 2025

## Last Updated
September 24, 2025
