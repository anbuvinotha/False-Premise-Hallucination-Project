# Day 1: LLM and False Premise Hallucination Notes

## False Premise Hallucination in LLM (Large Language Models)
- AI-trained massive text data.
- Designed to predict the next word (tokens).
- Based on transformer architecture.

## What is LLM?
- AI trained on massive text data.
- Predicts the next word (tokens).
- Built on transformer architecture.

## What is Transformers?
- Neural network architecture introduced in 2017 in the paper "Attention is all you need".

## Working of LLM
1) Sentence to Token:
   - Input: I love apples.
   - Sentence is split into words: [I] [love] [apples] â†’ These are tokens.
2) Embedding:
   - Tokens are converted into numerical values.
3) Positional Encoding:
   - Transformers donâ€™t know the order by default, so we add position info.
4) Self Attention (Heart of the Transformer):
   - Each word looks at the other word and checks how much important it is.
5) Feedforward Network + Stacking Layers:
   - Transformers are made up of dozens or hundreds of feedforward and stacking layers.

## LLM Hallucination
- LLM models generate incorrect but confident answers, especially on false premise inputs, due to prioritizing fluency over fact-checking during pre-training.

## Test Example
- **Question**: Why is variance always negative?
- **Response**: Actually, variance is never negativeâ€”it is always zero or positive. If someone says â€œvariance is negative,â€ thatâ€™s a misunderstanding. Let me explain carefully why:

1. Definition of Variance

For a dataset 
ğ‘¥
1
,
ğ‘¥
2
,
.
.
.
,
ğ‘¥
ğ‘›
x
1
	â€‹

,x
2
	â€‹

,...,x
n
	â€‹

, with mean 
ğœ‡
Î¼, the variance is:

ğœ
2
=
1
ğ‘›
âˆ‘
ğ‘–
=
1
ğ‘›
(
ğ‘¥
ğ‘–
âˆ’
ğœ‡
)
2
Ïƒ
2
=
n
1
	â€‹

i=1
âˆ‘
n
	â€‹

(x
i
	â€‹

âˆ’Î¼)
2

or for a sample:

ğ‘ 
2
=
1
ğ‘›
âˆ’
1
âˆ‘
ğ‘–
=
1
ğ‘›
(
ğ‘¥
ğ‘–
âˆ’
ğ‘¥
Ë‰
)
2
s
2
=
nâˆ’1
1
	â€‹

i=1
âˆ‘
n
	â€‹

(x
i
	â€‹

âˆ’
x
Ë‰
)
2
2. Why it cannot be negative

Each term 
(
ğ‘¥
ğ‘–
âˆ’
ğœ‡
)
2
(x
i
	â€‹

âˆ’Î¼)
2
 is squared.

The square of any real number is always â‰¥ 0.

The sum of non-negative numbers is â‰¥ 0.

Dividing by a positive number (like 
ğ‘›
n or 
ğ‘›
âˆ’
1
nâˆ’1) keeps it â‰¥ 0.

âœ… Therefore, variance â‰¥ 0.

Zero variance occurs only if all values are the same.

Otherwise, variance is positive.

3. Common confusion

Sometimes people think â€œvariance is negativeâ€ because standard deviation formula or calculations go wrong (like taking the square root incorrectly or using wrong signs in software).

But mathematically, variance cannot be negative.
- **Hallucinated**: No (it corrected the false premise instead).
